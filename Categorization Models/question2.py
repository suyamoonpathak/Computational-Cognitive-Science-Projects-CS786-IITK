
# Implementation of Anderson's venerable "rational" model of categorization.
# Assumes that stimuli were generated by a mixture of Gaussian distributions;
# rather than compute the full Bayesian posterior, it views items sequentially
# and assigns each to the maximum a posteriori cluster.
#
# At the end it is presented with a stimulus with one item missing, and
# predicts the probability that its value is a '0' or a '1'.
#
# Implemented in python by John McDonnell (Modified by Suyamoon Pathak (241110091) as a part of CSD Assignment 4)
#
# References: Anderson (1990) and Anderson (1991),

import numpy as np
import pandas as pd
from random import shuffle

# Utility functions:

class dLocalMAP:
    """
    Modified to handle continuous features (e.g., weight and height).
    """

    def __init__(self, args):
        self.partition = [[]]  
        self.c, self.alpha = args
        self.alpha0 = np.sum(self.alpha)
        self.N = 0 

    def probClustVal(self, k, feature_index, value):
        values_in_cluster = [x[feature_index] for x in self.partition[k]]
        if len(values_in_cluster) == 0:
            return 1 / np.sqrt(2 * np.pi * self.alpha[feature_index] ** 2)
        mean = np.mean(values_in_cluster)
        variance = np.var(values_in_cluster) + self.alpha[feature_index] 
        return (1 / np.sqrt(2 * np.pi * variance)) * np.exp(-((value - mean) ** 2) / (2 * variance))

    def condclusterprob(self, stim, k):
        return np.prod([self.probClustVal(k, i, stim[i]) for i in range(len(stim))])

    def posterior(self, stim):
        pk = np.zeros(len(self.partition))
        pFk = np.zeros(len(self.partition))

        for k in range(len(self.partition)):
            pk[k] = self.c * len(self.partition[k]) / ((1 - self.c) + self.c * self.N)
            if len(self.partition[k]) == 0:
                pk[k] = (1 - self.c) / ((1 - self.c) + self.c * self.N)
            pFk[k] = self.condclusterprob(stim, k)

        pkF = pk * pFk
        return pkF / np.sum(pkF) if np.sum(pkF) > 0 else pkF

    def stimulate(self, stim):
        cluster_probabilities = self.posterior(stim)
        winner = np.argmax(cluster_probabilities)

        if len(self.partition[winner]) == 0:
            self.partition.append([])
        self.partition[winner].append(stim)

        self.N += 1

    def predict(self, stim):
        cluster_probabilities = self.posterior(stim)
        return np.argmax(cluster_probabilities)


def predict_categories(training_data, test_data):
    model = dLocalMAP([0.5, np.ones(training_data.shape[1] - 1)])  #c = 0.5, alpha = [1, 1]
    
    for i in range(len(training_data)):
        model.stimulate(training_data[i, :-1]) 

    predictions = [model.predict(test_data[i]) + 1 for i in range(len(test_data))]
    return predictions

train_data = pd.read_csv('X.csv', header=None).values
test_data = pd.read_csv('y.csv', header=None).values

predictions = predict_categories(train_data, test_data)

output = pd.DataFrame(test_data, columns=['Weight', 'Height'])
output['Predicted_Category'] = predictions
output.to_csv('question2_y_with_rational_model_predictions.csv', index=False)

print("Predictions saved to question2_y_with_rational_model_predictions.csv")
